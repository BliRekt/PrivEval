{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "#Code for generating a baby dataset\n",
    "\n",
    "\n",
    "\n",
    "name_gen = Faker()\n",
    "num_samples = 9\n",
    "\n",
    "height = list(np.random.normal(loc=50, scale=1, size=num_samples))\n",
    "\n",
    "classic_icecreams = [\n",
    "    \"Vanilla\", \"Chocolate\", \"Strawberry\", \"Mint Chocolate Chip\",\n",
    "    \"Cookies and Cream\", \"Rocky Road\", \"Butter Pecan\", \"Neapolitan\",\n",
    "    \"Pistachio\", \"French Vanilla\"\n",
    "]\n",
    "\n",
    "fav_icecream = list(random.choices(classic_icecreams, k=num_samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random first and last names\n",
    "name_df = pd.DataFrame({\n",
    "    'First_Name': [name_gen.first_name() for _ in range(num_samples)],\n",
    "    'Last_Name': [name_gen.last_name() for _ in range(num_samples)]\n",
    "})\n",
    "\n",
    "height_df = pd.DataFrame({'Height': height})\n",
    "\n",
    "icecream_df = pd.DataFrame({'Flavour': fav_icecream})\n",
    "\n",
    "full_df = pd.concat([name_df, height_df, icecream_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from saiph.projection import fit_transform\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "sens_individual = ['John', 'Davies', 182.5, 'Vanilla']\n",
    "new_individual_df = pd.DataFrame([sens_individual], columns=full_df.columns)\n",
    "all_individuals = pd.concat([full_df, new_individual_df], ignore_index=True)\n",
    "print(all_individuals)\n",
    "\n",
    "\n",
    "#PCA\n",
    "coord_real_pca, model_pca = fit_transform(all_individuals, nf=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a synthetic dataset\n",
    "from synthesis.synthesizers.privbayes import PrivBayes\n",
    "from saiph.projection import transform\n",
    "\n",
    "eps=np.inf\n",
    "epsilon = eps\n",
    "\n",
    "pb = PrivBayes(epsilon=epsilon, verbose=False)\n",
    "\n",
    "pb.fit(all_individuals)\n",
    "\n",
    "gen_data  = pb.sample()\n",
    "\n",
    "final_data = pd.DataFrame(gen_data.values, columns=gen_data.columns, index=range(all_individuals.shape[0]))\n",
    "\n",
    "print(final_data)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "def scatter_plot(coord_real, coord_synth):\n",
    "    # Scatter Plot\n",
    "    plt.figure()\n",
    "\n",
    "    # Plot DataFrame 1\n",
    "    plt.scatter(coord_real['Dim. 1'], coord_real['Dim. 2'], color='blue', label='Real', alpha=0.7)\n",
    "\n",
    "    # Plot DataFrame 2\n",
    "    plt.scatter(coord_synth['Dim. 1'], coord_synth['Dim. 2'], color='red', label='Synthetic', alpha=0.5)\n",
    "\n",
    "    plt.title('Scatter Plot of real and synthetic data')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Show Plot\n",
    "    plt.show()\n",
    "\n",
    "#PCA\n",
    "syn_coords_pca = transform(final_data, model_pca)\n",
    "scatter_plot(coord_real_pca, syn_coords_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "#Import metrics\n",
    "from Metrics import All_synthcity\n",
    "\n",
    "all_individuals['Height'].astype('Float32')\n",
    "final_data['Height'].astype('Float32')\n",
    "\n",
    "le = LabelEncoder()\n",
    "r_fn = le.fit_transform(all_individuals['First_Name'])\n",
    "r_ln = le.fit_transform(all_individuals['Last_Name'])\n",
    "r_fl = le.fit_transform(all_individuals['Flavour'])\n",
    "x_ = pd.DataFrame({'First_Name':r_fn, 'Last_Name': r_ln, 'Height': final_data['Height'],'Flavour':r_fl})\n",
    "\n",
    "s_fn = le.fit_transform(final_data['First_Name'])\n",
    "s_ln = le.fit_transform(final_data['Last_Name'])\n",
    "s_fl = le.fit_transform(final_data['Flavour'])\n",
    "z_ = pd.DataFrame({'First_Name':s_fn, 'Last_Name': s_ln, 'Height': final_data['Height'],'Flavour':s_fl})\n",
    "\n",
    "\n",
    "\n",
    "metrics = {\n",
    "                'sanity': ['common_rows_proportion', 'nearest_syn_neighbor_distance', 'close_values_probability', 'distant_values_probability'],\n",
    "                'stats': ['alpha_precision'],\n",
    "                'detection': ['detection_mlp'],\n",
    "                'privacy': ['identifiability_score'],\n",
    "            }\n",
    "\n",
    "synthcity_results = All_synthcity.calculate_metric(args = None, _real_data=x_, _synthetic=z_, _metrics=metrics)\n",
    "\n",
    "crp = synthcity_results['mean'][1]\n",
    "nsnd = 1-synthcity_results['mean'][2]\n",
    "cvp = synthcity_results['mean'][3]\n",
    "dvp = 1-synthcity_results['mean'][4]\n",
    "auth = synthcity_results['mean'][10]\n",
    "mlp = synthcity_results['mean'][11]\n",
    "id_score = synthcity_results['mean'][12]\n",
    "\n",
    "from Metrics import AttributeInference as AIR\n",
    "from Metrics import CGeneralizedCAP as GCAP\n",
    "from Metrics import CZeroCAP as CZCAP\n",
    "from Metrics import DCR\n",
    "from Metrics import NNDR\n",
    "from Metrics import Hidden_rate\n",
    "from Metrics import NNAA\n",
    "from Metrics import MemInf as MIR\n",
    "from Metrics import Hitting_rate\n",
    "from Metrics import MDCR\n",
    "\n",
    "\n",
    "\n",
    "air = AIR.calculate_metric(args = None, _real_data=all_individuals, _synthetic=final_data)\n",
    "gcap = GCAP.calculate_metric(args = None, _real_data=x_, _synthetic=z_)\n",
    "zcap = CZCAP.calculate_metric(args = None, _real_data=x_, _synthetic=z_)\n",
    "mdcr = MDCR.calculate_metric(args=None, _real_data=x_, _synthetic=z_)\n",
    "hitR = Hitting_rate.calculate_metric(args=None, _real_data=x_, _synthetic=z_)\n",
    "mir = MIR.calculate_metric(args=None, _real_data=x_, _synthetic=z_)\n",
    "nnaa = NNAA.calculate_metric(args=None, _real_data=x_, _synthetic=z_)\n",
    "# dcr, nndr, hidR = calculate_dcr_nndr_hidr(args=None, _real_data=x_, _synthetic=z_)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "for i in range(10):\n",
    "    clear_output(wait=True)\n",
    "\n",
    "results = {'AIR':round(air, 2),'GCAP': round(gcap, 2), 'ZCAP':round(zcap, 2), \n",
    "           'MDCR':round(mdcr, 2), 'Hitting_rate':round(hitR, 2), 'MIR':round(mir, 2), \n",
    "           'NNAA':round(nnaa, 2), 'CRP':round(crp, 2), 'NSND':round(nsnd, 2), \n",
    "           'CVP':round(cvp, 2), 'DVP':round(dvp, 2), 'Authenticity':round(auth, 2), \n",
    "           'DetectionMLP':round(mlp, 2), 'Identifiability_score':round(id_score, 2)}\n",
    "\n",
    "\n",
    "print(results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code modified from https://github.com/octopize/avatar-paper\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "from saiph.projection import fit_transform\n",
    "from saiph.projection import transform\n",
    "from math import log\n",
    "from typing import Tuple, Union\n",
    "import faiss\n",
    "import math\n",
    "from numpy.typing import NDArray\n",
    "from numpy import bool_\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def calculate_dcr_nndr_hidr(args, _real_data, _synthetic):\n",
    "    real_data = _real_data\n",
    "    synthetic = _synthetic\n",
    "\n",
    "    # cat_vars = ['First_Name', 'Last_Name', 'Flavour']\n",
    "    # real_data[cat_vars].astype(\"category\")\n",
    "    # real_data['Height'].astype(np.float32)\n",
    "    \n",
    "    # #Assign categorical values with type \"category\" for both real and synthetic data\n",
    "    # categorical_val_real, continuous_val_real = get_categorical_continuous(real_data)\n",
    "    # real_data[categorical_val_real] = real_data[categorical_val_real].astype(\"category\")\n",
    "    \n",
    "    # categorical_val_synth, continous_val_synth = get_categorical_continuous(synthetic)\n",
    "    # synthetic[categorical_val_synth] = synthetic[categorical_val_synth].astype(\"category\")\n",
    "\n",
    "    #Fit model and get coordinates\n",
    "    real_coord, model = fit_transform(real_data, nf=2)\n",
    "    synth_coord = transform(synthetic, model)\n",
    "\n",
    "    dcr = statistics.mean(get_dcr(real_coord, synth_coord))\n",
    "    \n",
    "    if dcr == 0:\n",
    "        dcr = 1\n",
    "    else:\n",
    "        dcr = 1 - sigmoid(log(dcr, 10))\n",
    "    \n",
    "    nndr = 1 - statistics.mean(get_nndr(real_coord, synth_coord))\n",
    "    \n",
    "    are_first_hit = avatars_are_k_hit(\n",
    "            real_coord, synth_coord, distance_metric=\"minkowski\", k=1\n",
    "        )\n",
    "    \n",
    "    hiddr = calc_hidden_rate(are_first_hit)\n",
    "    \n",
    "    return dcr, nndr, hiddr\n",
    "    \n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x)) \n",
    "\n",
    "#Return column names of the categorical(strings) and continuous(integers) attributes\n",
    "def get_categorical_continuous(data: pd.DataFrame):\n",
    "    categorical_val = []\n",
    "    cont_val = []\n",
    "    for column in data.columns:\n",
    "            if data[column].dtype == 'O':\n",
    "                categorical_val.append(column)\n",
    "            else:\n",
    "                cont_val.append(column)\n",
    "    \n",
    "    return categorical_val, cont_val\n",
    "\n",
    "\n",
    "def get_dcr(train_coord, synth_coord):\n",
    "    \"\"\"Get distances to the closest records.\n",
    "\n",
    "    DCR is the distance of each synthetic record to a record in the original dataset.\n",
    "    \"\"\"\n",
    "    indices_distances = get_distances_closest_records(\n",
    "        train_coord, synth_coord, searching_frame=1\n",
    "    )\n",
    "    _, distances = zip(*indices_distances)\n",
    "    return [distance[0] for distance in distances]\n",
    "\n",
    "def get_nndr(train_coord, synth_coord):\n",
    "    \"\"\"Get nearest neighbors distance ratio.\n",
    "\n",
    "    Ratio of the distance of each synthetic record to its closest\n",
    "    to the second closest record in the original dataset.\n",
    "    \"\"\"\n",
    "    indices_distances = get_distances_closest_records(\n",
    "        train_coord, synth_coord, searching_frame=2\n",
    "    )\n",
    "    _, distances = zip(*indices_distances)\n",
    "\n",
    "    ratio = [\n",
    "        1 if distance[1] == 0 else distance[0] / distance[1] for distance in distances\n",
    "    ]\n",
    "    return ratio\n",
    "\n",
    "def calc_hidden_rate(are_first_hit: NDArray[bool_]) -> float:\n",
    "    \"\"\"Test for each record if the nearest avatar is the one generated by the original record itself.\n",
    "\n",
    "    Arguments:\n",
    "        are_first_hit: whether the nearest neighbor of the supplied\n",
    "          original record is the avatar created from this record\n",
    "\n",
    "    Returns:\n",
    "        float: the percentage of records that could be considered \"safe\"\n",
    "    \"\"\"\n",
    "    res: float = (are_first_hit.sum() / len(are_first_hit))\n",
    "    return res\n",
    "\n",
    "def avatars_are_k_hit(\n",
    "    records_coordinates: pd.DataFrame,\n",
    "    avatars_coordinates: pd.DataFrame,\n",
    "    distance_metric: str = \"minkowski\",\n",
    "    k: int = 1,\n",
    ") -> NDArray[np.bool_]:\n",
    "    \"\"\"Test for each record if the nearest avatar is the one generated by the original record itself.\n",
    "\n",
    "    Arguments:\n",
    "        records_coordinates\n",
    "        avatars_coordinates\n",
    "        distance_metric\n",
    "        k\n",
    "\n",
    "    Returns:\n",
    "        array: whether the nearest neighbor of the supplied original\n",
    "          record is the avatar created from this record\n",
    "    \"\"\"\n",
    "    if distance_metric not in [\n",
    "        \"cityblock\",\n",
    "        \"cosine\",\n",
    "        \"euclidean\",\n",
    "        \"l1\",\n",
    "        \"l2\",\n",
    "        \"manhattan\",\n",
    "        \"braycurtis\",\n",
    "        \"canberra\",\n",
    "        \"chebyshev\",\n",
    "        \"correlation\",\n",
    "        \"dice\",\n",
    "        \"hamming\",\n",
    "        \"jaccard\",\n",
    "        \"kulsinski\",\n",
    "        \"mahalanobis\",\n",
    "        \"minkowski\",\n",
    "        \"rogerstanimoto\",\n",
    "        \"russellrao\",\n",
    "        \"seuclidean\",\n",
    "        \"sokalmichener\",\n",
    "        \"sokalsneath\",\n",
    "        \"sqeuclidean\",\n",
    "        \"yule\",\n",
    "    ]:\n",
    "        raise ValueError(\"distance_metric\", \"invalid distance metric\")\n",
    "\n",
    "    #if records_coordinates.shape[0] != avatars_coordinates.shape[0]:\n",
    "    #    raise ValueError(\n",
    "    #        \"dimension\",\n",
    "    #        \"Records set and synthetic set dataframes must have the same number of observations\",\n",
    "    #    )\n",
    "\n",
    "    if distance_metric == \"minkowski\":\n",
    "        nn = FaissKNeighbors(k=k)\n",
    "        nn.fit(np.array(avatars_coordinates))\n",
    "        distances, indices = nn.predict(np.array(records_coordinates))\n",
    "    else:\n",
    "        nn = NearestNeighbors(\n",
    "            n_neighbors=1, algorithm=\"ball_tree\", metric=distance_metric\n",
    "        )\n",
    "        nn.fit(avatars_coordinates)\n",
    "        distances, indices = nn.kneighbors(records_coordinates)  # type: ignore\n",
    "\n",
    "    indices_array: NDArray[np.int_] = np.array([item[0] for item in indices])\n",
    "\n",
    "    are_k_hit: NDArray[np.bool_] = np.equal(\n",
    "        indices_array, range(0, records_coordinates.shape[0])\n",
    "    )\n",
    "\n",
    "    return are_k_hit\n",
    "\n",
    "def get_distances_closest_records(\n",
    "    records: pd.DataFrame, synthetic: pd.DataFrame, searching_frame: int\n",
    "):\n",
    "    \"\"\"Get index and distances of the closest records.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "        records: Original records\n",
    "        synthetic: Synthetic data\n",
    "        searching_frame: number of neighbors to find\n",
    "    Returns\n",
    "    -------\n",
    "        indices_distances: indices, distances nearest neighbor among original records.\n",
    "\n",
    "    \"\"\"\n",
    "    nn = FaissKNeighbors(k=searching_frame)\n",
    "    nn.fit(np.array(records))\n",
    "\n",
    "    # index.search returns two arrays (distances, indices)\n",
    "    # https://github.com/facebookresearch/faiss/wiki/Getting-started\n",
    "    distances, indices = nn.predict(synthetic.to_numpy().astype(np.float32))\n",
    "\n",
    "    indices_distances = list(zip(indices, distances))\n",
    "    return indices_distances\n",
    "\n",
    "class FaissKNeighbors:\n",
    "    index: Union[faiss.IndexFlatL2, faiss.IndexIVFFlat]\n",
    "\n",
    "    def __init__(self, k: int = 5) -> None:\n",
    "        self.index = faiss.IndexFlatL2()\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X: NDArray[np.float_]) -> None:\n",
    "        xb: NDArray[np.float_] = X.astype(np.float32)\n",
    "        size, dimension = X.shape\n",
    "        nlist = round(math.sqrt(size))\n",
    "        threshold_size = 50000\n",
    "\n",
    "        # Use Index for large size dataframe\n",
    "        if size > threshold_size:\n",
    "            quantizer = faiss.IndexFlatL2(dimension)\n",
    "            self.index = faiss.IndexIVFFlat(\n",
    "                quantizer, dimension, nlist, faiss.METRIC_L2\n",
    "            )\n",
    "            assert self.index is not None  # nosec: B101\n",
    "            self.index.train(xb)\n",
    "\n",
    "        # perform exhaustive search otherwise\n",
    "        else:\n",
    "            self.index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "        assert self.index is not None  # nosec: B101\n",
    "        self.index.add(xb)\n",
    "\n",
    "    def predict(\n",
    "        self, X: NDArray[np.float_]\n",
    "    ) -> Tuple[NDArray[np.float_], NDArray[np.int_]]:\n",
    "        assert self.index is not None  # nosec: B101\n",
    "        distances, indices = self.index.search(X.astype(np.float32), k=self.k)\n",
    "        distances = np.sqrt(distances)\n",
    "        return distances, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scatter_plot_tsne(coord_real, coord_synth):\n",
    "    real = pd.DataFrame(coord_real)\n",
    "    syn = pd.DataFrame(coord_synth)\n",
    "    coords = pd.merge(real, syn)\n",
    "    # Scatter Plot\n",
    "    plt.figure()\n",
    "\n",
    "    # Plot DataFrame 1\n",
    "    plt.scatter(real[0], real[1], color='blue', label='Real', alpha=0.5)\n",
    "\n",
    "    # Plot DataFrame 2\n",
    "    plt.scatter(syn[0], syn[1], color='red', label='Synthetic', alpha=0.5)\n",
    "\n",
    "    plt.title('Scatter Plot of real and synthetic data')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Show Plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "\n",
    "tsne_coord_real = tsne.fit_transform(all_individuals)\n",
    "\n",
    "tsne_coord_syn = tsne.fit_transform(final_data)\n",
    "\n",
    "scatter_plot_tsne(tsne_coord_real, tsne_coord_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from saiph.projection import fit_transform\n",
    "from saiph.projection import transform\n",
    "\n",
    "def scatter_plot(coord_real, coord_synth):\n",
    "    # Scatter Plot\n",
    "    plt.figure()\n",
    "\n",
    "    # Plot DataFrame 1\n",
    "    plt.scatter(coord_real['Dim. 1'], coord_real['Dim. 2'], color='blue', label='Real', alpha=0.7)\n",
    "\n",
    "    # Plot DataFrame 2\n",
    "    plt.scatter(coord_synth['Dim. 1'], coord_synth['Dim. 2'], color='red', label='Synthetic', alpha=0.5)\n",
    "\n",
    "    plt.title('Scatter Plot of real and synthetic data')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Show Plot\n",
    "    plt.show()\n",
    "    \n",
    "pca_coord_real, model = fit_transform(all_individuals, nf=2)\n",
    "pca_coord_syn = transform(final_data, model)\n",
    "\n",
    "scatter_plot(pca_coord_real, pca_coord_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Constructing Bayesian Network (BN) ================\n",
      "Adding ROOT Times Been to Italy\n",
      "Adding attribute Last Name\n",
      "Adding attribute Nationality\n",
      "Adding attribute First Time London\n",
      "Adding attribute First Name\n",
      "Adding attribute Steps per Day\n",
      "Adding attribute Favorite Icecream\n",
      "Adding attribute Height\n",
      "Adding attribute Like Liquorice\n",
      "========================== BN constructed ==========================\n",
      "================ Constructing Bayesian Network (BN) ================\n",
      "Adding ROOT Times Been to Italy\n",
      "Adding attribute Last Name\n",
      "Adding attribute First Name\n",
      "Adding attribute Steps per Day\n",
      "Adding attribute First Time London\n",
      "Adding attribute Height\n",
      "Adding attribute Favorite Icecream\n",
      "Adding attribute Like Liquorice\n",
      "Adding attribute Nationality\n",
      "========================== BN constructed ==========================\n",
      "================ Constructing Bayesian Network (BN) ================\n",
      "Adding ROOT Times Been to Italy\n",
      "Adding attribute Nationality\n",
      "Adding attribute First Time London\n",
      "Adding attribute Height\n",
      "Adding attribute Favorite Icecream\n",
      "Adding attribute Like Liquorice\n",
      "Adding attribute First Name\n",
      "Adding attribute Steps per Day\n",
      "Adding attribute Last Name\n",
      "========================== BN constructed ==========================\n",
      "================ Constructing Bayesian Network (BN) ================\n",
      "Adding ROOT Times Been to Italy\n",
      "Adding attribute Nationality\n",
      "Adding attribute Steps per Day\n",
      "Adding attribute Like Liquorice\n",
      "Adding attribute First Time London\n",
      "Adding attribute First Name\n",
      "Adding attribute Last Name\n",
      "Adding attribute Favorite Icecream\n",
      "Adding attribute Height\n",
      "========================== BN constructed ==========================\n",
      "================ Constructing Bayesian Network (BN) ================\n",
      "Adding ROOT Times Been to Italy\n",
      "Adding attribute Height\n",
      "Adding attribute Steps per Day\n",
      "Adding attribute Favorite Icecream\n",
      "Adding attribute Nationality\n",
      "Adding attribute First Name\n",
      "Adding attribute Last Name\n",
      "Adding attribute First Time London\n",
      "Adding attribute Like Liquorice\n",
      "========================== BN constructed ==========================\n",
      "================ Constructing Bayesian Network (BN) ================\n",
      "Adding ROOT Times Been to Italy\n",
      "Adding attribute Like Liquorice\n",
      "Adding attribute Steps per Day\n",
      "Adding attribute First Name\n",
      "Adding attribute Height\n",
      "Adding attribute Last Name\n",
      "Adding attribute Nationality\n",
      "Adding attribute Favorite Icecream\n",
      "Adding attribute First Time London\n",
      "========================== BN constructed ==========================\n",
      "================ Constructing Bayesian Network (BN) ================\n",
      "Adding ROOT Times Been to Italy\n",
      "Adding attribute Steps per Day\n",
      "Adding attribute Last Name\n",
      "Adding attribute First Name\n",
      "Adding attribute Nationality\n",
      "Adding attribute Height\n",
      "Adding attribute Favorite Icecream\n",
      "Adding attribute Like Liquorice\n",
      "Adding attribute First Time London\n",
      "========================== BN constructed ==========================\n",
      "================ Constructing Bayesian Network (BN) ================\n",
      "Adding ROOT Times Been to Italy\n",
      "Adding attribute Steps per Day\n",
      "Adding attribute Last Name\n",
      "Adding attribute First Name\n",
      "Adding attribute Nationality\n",
      "Adding attribute Height\n",
      "Adding attribute Favorite Icecream\n",
      "Adding attribute Like Liquorice\n",
      "Adding attribute First Time London\n",
      "========================== BN constructed ==========================\n",
      "================ Constructing Bayesian Network (BN) ================\n",
      "Adding ROOT Times Been to Italy\n",
      "Adding attribute Last Name\n",
      "Adding attribute Nationality\n",
      "Adding attribute First Name\n",
      "Adding attribute Like Liquorice\n",
      "Adding attribute Steps per Day\n",
      "Adding attribute Height\n",
      "Adding attribute Favorite Icecream\n",
      "Adding attribute First Time London\n",
      "========================== BN constructed ==========================\n",
      "================ Constructing Bayesian Network (BN) ================\n",
      "Adding ROOT Times Been to Italy\n",
      "Adding attribute First Name\n",
      "Adding attribute Last Name\n",
      "Adding attribute Height\n",
      "Adding attribute Favorite Icecream\n",
      "Adding attribute Steps per Day\n",
      "Adding attribute Nationality\n",
      "Adding attribute Like Liquorice\n",
      "Adding attribute First Time London\n",
      "========================== BN constructed ==========================\n",
      "================ Constructing Bayesian Network (BN) ================\n",
      "Adding ROOT Times Been to Italy\n",
      "Adding attribute Like Liquorice\n",
      "Adding attribute Nationality\n",
      "Adding attribute Steps per Day\n",
      "Adding attribute Favorite Icecream\n",
      "Adding attribute Last Name\n",
      "Adding attribute First Name\n",
      "Adding attribute First Time London\n",
      "Adding attribute Height\n",
      "========================== BN constructed ==========================\n",
      "================ Constructing Bayesian Network (BN) ================\n",
      "Adding ROOT Times Been to Italy\n",
      "Adding attribute Last Name\n",
      "Adding attribute Like Liquorice\n",
      "Adding attribute Steps per Day\n",
      "Adding attribute First Name\n",
      "Adding attribute First Time London\n",
      "Adding attribute Height\n",
      "Adding attribute Favorite Icecream\n",
      "Adding attribute Nationality\n",
      "========================== BN constructed ==========================\n",
      "================ Constructing Bayesian Network (BN) ================\n",
      "Adding ROOT Times Been to Italy\n",
      "Adding attribute Steps per Day\n",
      "Adding attribute Height\n",
      "Adding attribute Last Name\n",
      "Adding attribute First Name\n",
      "Adding attribute Like Liquorice\n",
      "Adding attribute Nationality\n",
      "Adding attribute Favorite Icecream\n",
      "Adding attribute First Time London\n",
      "========================== BN constructed ==========================\n",
      "================ Constructing Bayesian Network (BN) ================\n",
      "Adding ROOT Times Been to Italy\n",
      "Adding attribute Steps per Day\n",
      "Adding attribute Nationality\n",
      "Adding attribute First Name\n",
      "Adding attribute Last Name\n",
      "Adding attribute Height\n",
      "Adding attribute Like Liquorice\n",
      "Adding attribute Favorite Icecream\n",
      "Adding attribute First Time London\n",
      "========================== BN constructed ==========================\n",
      "================ Constructing Bayesian Network (BN) ================\n",
      "Adding ROOT Times Been to Italy\n",
      "Adding attribute Steps per Day\n",
      "Adding attribute Last Name\n",
      "Adding attribute First Name\n",
      "Adding attribute Height\n",
      "Adding attribute Nationality\n",
      "Adding attribute Favorite Icecream\n",
      "Adding attribute First Time London\n",
      "Adding attribute Like Liquorice\n",
      "========================== BN constructed ==========================\n",
      "================ Constructing Bayesian Network (BN) ================\n",
      "Adding ROOT Times Been to Italy\n",
      "Adding attribute Steps per Day\n",
      "Adding attribute Last Name\n",
      "Adding attribute First Name\n",
      "Adding attribute Height\n",
      "Adding attribute Nationality\n",
      "Adding attribute Favorite Icecream\n",
      "Adding attribute Like Liquorice\n",
      "Adding attribute First Time London\n",
      "========================== BN constructed ==========================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import random\n",
    "from DataSynthesizer1.DataDescriber import DataDescriber\n",
    "from DataSynthesizer1.DataGenerator import DataGenerator\n",
    "from synthesis.synthesizers.privbayes import PrivBayes\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def generate_real_data(num_samples, liquorice):\n",
    "    #Code for generating a baby dataset\n",
    "    name_gen = Faker()\n",
    "    heights = np.around(list(np.random.normal(loc=170, scale=10, size=num_samples)), 2)\n",
    "    \n",
    "\n",
    "    # Generate random first and last names\n",
    "    name_df = pd.DataFrame({\n",
    "        'First_Name': [name_gen.first_name() for _ in range(num_samples)],\n",
    "        'Last_Name': [name_gen.last_name() for _ in range(num_samples)]\n",
    "    })\n",
    "    height_df = pd.DataFrame({'Height': heights})\n",
    "    basic_df = pd.concat([name_df, height_df], axis=1)\n",
    "\n",
    "    # Define country list and correlation rules\n",
    "    countries = [\"USA\", \"Canada\", \"Germany\", \"France\", \"Italy\", \"China\", \"Brazil\", \"Australia\", \"Japan\", \"UK\", \"Sweden\", \"Norway\", \"Denmark\", \"Finland\"]\n",
    "\n",
    "    # Ice cream preferences (default: random choice)\n",
    "    ice_creams = [\"Vanilla\", \"Chocolate\", \"Strawberry\", \"Mint\", \"Pistachio\", \"Stracciatella\"]\n",
    "\n",
    "    # Generate data\n",
    "    data = []\n",
    "    for i in range(num_samples):\n",
    "        person = {}\n",
    "\n",
    "        # Assign country\n",
    "        person[\"Country of Origin\"] = random.choice(countries)\n",
    "\n",
    "        # Assign favorite ice cream with correlation (Italy → Stracciatella preference)\n",
    "        if person[\"Country of Origin\"] == \"Italy\":\n",
    "            person[\"Favorite Ice Cream\"] = np.random.choice(ice_creams, p=[0.1, 0.1, 0.1, 0.1, 0.2, 0.4])\n",
    "        else:\n",
    "            person[\"Favorite Ice Cream\"] = random.choice(ice_creams)\n",
    "\n",
    "        # Assign liking for liquorice (Nordic countries → Higher probability)\n",
    "        if person[\"Country of Origin\"] in [\"Sweden\", \"Norway\", \"Denmark\", \"Finland\"]:\n",
    "            person[\"Likes Liquorice\"] = np.random.choice([1, 0], p=[0.9, 0.1])  # 70% chance for Nordic countries\n",
    "        else:\n",
    "            person[\"Likes Liquorice\"] = np.random.choice([1, 0], p=[0.2, 0.8])  # 20% for others\n",
    "\n",
    "        # Assign number of times visited Italy (Random integer, but higher if from Europe)\n",
    "        if person[\"Country of Origin\"] in [\"Germany\", \"France\", \"UK\", \"Sweden\", \"Norway\", \"Denmark\", \"Finland\", \"Italy\"]:\n",
    "            person[\"Times Visited Italy\"] = np.random.poisson(2)  # Higher average visits\n",
    "        else:\n",
    "            person[\"Times Visited Italy\"] = np.random.poisson(0.5)  # Lower average visits\n",
    "\n",
    "        # First time in London (UK residents more likely to say yes)\n",
    "        person[\"First Time in London\"] = 1 if person[\"Country of Origin\"] == \"UK\" else np.random.choice([1, 0], p=[0.2, 0.8])\n",
    "\n",
    "        # Number of steps per day (Normal distribution with realistic values)\n",
    "        person[\"Steps per Day\"] = max(1000, int(np.random.normal(8000, 3000)))  # Avoids negative steps\n",
    "\n",
    "        data.append(person)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    full_df = pd.concat([basic_df, df], axis=1, ignore_index=True)\n",
    "    \n",
    "    if liquorice == 0:\n",
    "        # Sample row: UK resident who does NOT like liquorice\n",
    "        indiv = [\n",
    "            \"James\", \"Smith\", round(random.gauss(175, 10), 2), \"UK\", \"Strawberry\", 0, 2, 0, 7500\n",
    "        ]\n",
    "        indiv_df = pd.DataFrame([indiv], columns=full_df.columns)\n",
    "    if liquorice == 1:\n",
    "        # Sample row: Sweden resident who LIKES liquorice\n",
    "        indiv = [\n",
    "            \"Lars\", \"Andersson\", round(random.gauss(185, 10), 2), \"Sweden\", \"Chocolate\", 1, 3, 0, 9200 \n",
    "        ]\n",
    "        indiv_df = pd.DataFrame([indiv], columns=full_df.columns)\n",
    "    full_df = pd.concat([full_df, indiv_df], ignore_index=True)\n",
    "    full_df.columns = [\"First Name\", \"Last Name\", \"Height\", \"Nationality\", \"Favorite Icecream\", \"Like Liquorice\", \"Times Been to Italy\", \"First Time London\", \"Steps per Day\"]\n",
    "    print(full_df)\n",
    "    # Save to CSV (optional)\n",
    "    full_df.to_csv(f\"sample_data_{liquorice}.csv\", index=False)\n",
    "\n",
    "def synthesize_no_bin(real_data, eps):    \n",
    "    # instantiate and fit synthesizer\n",
    "    pb = PrivBayes(epsilon=eps, verbose=False)\n",
    "    pb.fit(real_data)\n",
    "\n",
    "    # Synthesize data\n",
    "    gen_data  = pb.sample()\n",
    "\n",
    "    # Save to csv file\n",
    "    result = pd.DataFrame(gen_data.values, columns=gen_data.columns, index=range(real_data.shape[0]))\n",
    "    \n",
    "    return result\n",
    "\n",
    "def synthesize_bin(real_data, eps):\n",
    "    \n",
    "    fn_encoder = LabelEncoder()\n",
    "    ln_encoder = LabelEncoder()\n",
    "    fl_encoder = LabelEncoder()\n",
    "    na_encoder = LabelEncoder()\n",
    "    r_fn = fn_encoder.fit_transform(real_data['First Name'])\n",
    "    r_ln = ln_encoder.fit_transform(real_data['Last Name'])\n",
    "    r_na = na_encoder.fit_transform(real_data['Nationality'])\n",
    "    r_fl = fl_encoder.fit_transform(real_data['Favorite Icecream'])\n",
    "    all_labels = pd.DataFrame({'First Name':r_fn, 'Last Name': r_ln, 'Height': real_data['Height'],'Nationality': r_na, 'Favorite Icecream':r_fl, 'Like Liquorice': real_data['Like Liquorice'], 'Times Been to Italy': real_data['Times Been to Italy'], 'First Time London': real_data['First Time London'], 'Steps per Day': real_data['Steps per Day']})\n",
    "    \n",
    "    \n",
    "    describer = DataDescriber()\n",
    "    timestamp = datetime.now().timestamp()\n",
    "    all_labels.to_csv(f'{timestamp}.csv', index=False)\n",
    "    describer.describe_dataset_in_correlated_attribute_mode(dataset_file=f'{timestamp}.csv', \n",
    "                                                            epsilon=eps, \n",
    "                                                            k=2,\n",
    "                                                            attribute_to_is_candidate_key={\"First Name\": False, \"Last Name\": False, \"Height\": False, \"Nationality\": False, \"Favorite Icecream\": False, \"Like Liquorice\": False, \"Times Been to Italy\": False, \"First Time London\": False, \"Steps per Day\": False},\n",
    "                                                            attribute_to_datatype={\"First_Name\": \"Integer\", \"Last_Name\": \"Integer\", \"Height\": \"Float\", \"Nationality\": \"Integer\",\"Favorite Icecream\": \"Integer\",\"Like Liquorice\": \"Integer\",\"Times Been to Italy\": \"Integer\", \"First Time London\": \"Integer\",\"Steps per Day\": \"Integer\"},\n",
    "                                                            attribute_to_is_categorical={'First_Name': True, 'Last_Name': True, 'Height': False, \"Nationality\": True, \"Favorite Icecream\": True,\"Like Liquorice\": True,\"Times Been to Italy\": True, \"First Time London\": True,\"Steps per Day\": True},\n",
    "                                                            )\n",
    "    description = f'{timestamp}.json'\n",
    "    syn_path = f'syn_{timestamp}.csv'\n",
    "    describer.save_dataset_description_to_file(description)\n",
    "    generator = DataGenerator()\n",
    "    generator.generate_dataset_in_correlated_attribute_mode(n=len(all_labels), description_file=description, seed=timestamp)\n",
    "    generator.save_synthetic_data(syn_path)\n",
    "    result = pd.read_csv(syn_path, index_col=False).round(2)\n",
    "    os.remove(f'{timestamp}.csv')\n",
    "    os.remove(f'{timestamp}.json')\n",
    "    os.remove(f'syn_{timestamp}.csv')\n",
    "    result['First Name'] = fn_encoder.inverse_transform(result['First Name'].astype(int))\n",
    "    result['Last Name'] = ln_encoder.inverse_transform(result['Last Name'].astype(int))\n",
    "    result['Nationality'] = na_encoder.inverse_transform(result['Nationality'].astype(int))\n",
    "    result['Favorite Icecream'] = fl_encoder.inverse_transform(result['Favorite Icecream'].astype(int))\n",
    "    return result\n",
    "\n",
    "    \n",
    "#generate_real_data(1499, 1)\n",
    "#generate_real_data(1499, 0)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    eps_list = [0.02, 0.05, 0.1, 0.2, 0.5, 1, 2.5, 5]\n",
    "    for liquorice in [0,1]:\n",
    "        real_dir = f'sample_data_{liquorice}.csv'\n",
    "        real_data = pd.read_csv(real_dir)\n",
    "        for eps in eps_list:\n",
    "            #synthesize_no_bin(real_data, eps).to_csv(f\"demo_syn/syn_bin_{liquorice}_{eps}.csv\")\n",
    "            synthesize_bin(real_data, eps).to_csv(f\"demo_syn_new/syn_no_{liquorice}_{eps}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Metrics import AttributeInference as AIR\n",
    "from Metrics import CGeneralizedCAP as GCAP\n",
    "from Metrics import CZeroCAP as CZCAP\n",
    "from Metrics import DCR\n",
    "from Metrics import NNDR\n",
    "from Metrics import Hidden_rate\n",
    "from Metrics import NNAA\n",
    "from Metrics import MemInf as MIR\n",
    "from Metrics import Hitting_rate\n",
    "from Metrics import MDCR\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from Metrics import All_synthcity\n",
    "import pandas as pd\n",
    "\n",
    "def get_metric_results(real_data, syn_data):\n",
    "    real_data['Height'].astype('Float32')\n",
    "    syn_data['Height'].astype('Float32')\n",
    "\n",
    "    all_data = pd.concat([real_data, syn_data])\n",
    "    fn_encoder = LabelEncoder()\n",
    "    ln_encoder = LabelEncoder()\n",
    "    fl_encoder = LabelEncoder()\n",
    "    na_encoder = LabelEncoder()\n",
    "    r_fn = fn_encoder.fit_transform(all_data['First Name'])\n",
    "    r_ln = ln_encoder.fit_transform(all_data['Last Name'])\n",
    "    r_na = na_encoder.fit_transform(all_data['Nationality'])\n",
    "    r_fl = fl_encoder.fit_transform(all_data['Favorite Icecream'])\n",
    "    all_labels = pd.DataFrame({'First Name':r_fn, 'Last Name': r_ln, 'Height': all_data['Height'],'Nationality': r_na, 'Favorite Icecream':r_fl, 'Like Liquorice': all_data['Like Liquorice'], 'Times Been to Italy': all_data['Times Been to Italy'], 'First Time London': all_data['First Time London'], 'Steps per Day': all_data['Steps per Day']})\n",
    "    real_labels = all_labels[:len(real_data)]\n",
    "    syn_labels = all_labels[-len(real_data):]\n",
    "    \n",
    "    metrics = {\n",
    "                    'sanity': ['common_rows_proportion', 'nearest_syn_neighbor_distance', 'close_values_probability', 'distant_values_probability'],\n",
    "                    'stats': ['alpha_precision'],\n",
    "                    'detection': ['detection_mlp'],\n",
    "                    'privacy': ['identifiability_score'],\n",
    "                }\n",
    "    #air = AIR.calculate_metric(args = None, _real_data=real_dat, _synthetic=syn_dat)\n",
    "    synthcity_results = All_synthcity.calculate_metric(args = None, _real_data=real_data, _synthetic=real_data, _metrics=metrics)\n",
    "    crp = synthcity_results['mean'][1]\n",
    "    nsnd = 1-synthcity_results['mean'][2]\n",
    "    cvp = synthcity_results['mean'][3]\n",
    "    dvp = 1-synthcity_results['mean'][4]\n",
    "    auth = synthcity_results['mean'][10]\n",
    "    mlp = synthcity_results['mean'][11]\n",
    "    id_score = synthcity_results['mean'][12]\n",
    "    air = AIR.calculate_metric(args = None, _real_data=real_data, _synthetic=syn_data)\n",
    "    gcap = GCAP.calculate_metric(args = None, _real_data=real_labels, _synthetic=syn_labels)\n",
    "    zcap = CZCAP.calculate_metric(args = None, _real_data=real_labels, _synthetic=syn_labels)\n",
    "    mdcr = MDCR.calculate_metric(args=None, _real_data=real_labels, _synthetic=syn_labels)\n",
    "    hitR = Hitting_rate.calculate_metric(args=None, _real_data=real_labels, _synthetic=syn_labels)\n",
    "    mir = MIR.calculate_metric(args=None, _real_data=real_labels, _synthetic=syn_labels)\n",
    "    nnaa = NNAA.calculate_metric(args=None, _real_data=real_labels, _synthetic=syn_labels)\n",
    "    #---These metrics simply take too long to run\n",
    "    dcr = DCR.calculate_metric(args=None, _real_data=real_labels, _synthetic=syn_labels)\n",
    "    nndr = NNDR.calculate_metric(args=None, _real_data=real_labels, _synthetic=syn_labels)\n",
    "    hidd = Hidden_rate.calculate_metric(args=None, _real_data=real_labels, _synthetic=syn_labels)\n",
    "     \n",
    "    priv_results = np.around([air, gcap, zcap, \n",
    "                            mdcr, hitR, mir, \n",
    "                            nnaa, crp, nsnd, \n",
    "                            cvp, dvp, auth, \n",
    "                            mlp, id_score, \n",
    "                            dcr, nndr, hidd\n",
    "                            ], 2).tolist()\n",
    "    \n",
    "    metric_list = [\"Attribute Inference Risk\", \"GeneralizedCAP\", \"ZeroCAP\", \n",
    "                   \"Median Distance to Closest Record\", \"Hitting Rate\",\n",
    "                   \"Membership Inference Risk\", \"Nearest Neighbour Adversarial Accuracy\",\n",
    "                   \"Common Row Proportion\", \"Nearest Synthetic Neighbour Distance\",\n",
    "                   \"Close Value Probability\", \"Distant Value Probability\",\n",
    "                   \"Authenticity\", \"DetectionMLP\", \"Identifiability Score\"\n",
    "                   , \"Distance to Closest Record\", \"Nearest Neighbour Distance Ratio\", \"Hidden Rate\"\n",
    "                   ]\n",
    "    \n",
    "    results = pd.DataFrame({'Metric':metric_list, 'Result':priv_results})\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "eps_list = [0.02, 0.05, 0.1, 0.2, 0.5, 1, 2.5, 5]\n",
    "for liquorice in [0,1]:\n",
    "    real_dir = f'sample_data_{liquorice}.csv'\n",
    "    real_data = pd.read_csv(real_dir)\n",
    "    for eps in eps_list:\n",
    "        syn_no = pd.read_csv(f\"demo_syn/syn_no_{liquorice}_{eps}.csv\")\n",
    "        syn_bin = pd.read_csv(f\"demo_syn/syn_bin_{liquorice}_{eps}.csv\")\n",
    "        get_metric_results(real_data, syn_no).to_csv(f\"metric_results/syn_bin_{liquorice}_{eps}.csv\")\n",
    "        get_metric_results(real_data, syn_bin).to_csv(f\"metric_results/syn_no_{liquorice}_{eps}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/core/indexes/range.py:413\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_range\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[0;31mValueError\u001b[0m: 1 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 87\u001b[0m\n\u001b[1;32m     85\u001b[0m syn_bin \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdemo_syn/syn_bin_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mliquorice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00meps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m#get_metric_results(real_data, syn_no).to_csv(f\"metric_full_new/syn_bin_{liquorice}_{eps}.csv\")\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m \u001b[43mget_metric_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msyn_bin\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric_full_new/syn_no_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mliquorice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00meps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 38\u001b[0m, in \u001b[0;36mget_metric_results\u001b[0;34m(real_data, syn_data)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#air = AIR.calculate_metric(args = None, _real_data=real_data, _synthetic=syn_data)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m synthcity_results \u001b[38;5;241m=\u001b[39m All_synthcity\u001b[38;5;241m.\u001b[39mcalculate_metric(args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, _real_data\u001b[38;5;241m=\u001b[39mreal_data, _synthetic\u001b[38;5;241m=\u001b[39msyn_data, _metrics\u001b[38;5;241m=\u001b[39mmetrics)\n\u001b[0;32m---> 38\u001b[0m crp \u001b[38;5;241m=\u001b[39m \u001b[43msynthcity_results\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     39\u001b[0m nsnd \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39msynthcity_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     40\u001b[0m cvp \u001b[38;5;241m=\u001b[39m synthcity_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m3\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/core/indexes/range.py:415\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_range\u001b[38;5;241m.\u001b[39mindex(new_key)\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "from Metrics import All_synthcity\n",
    "from Metrics import AttributeInference1 as AIR\n",
    "from Metrics import CGeneralizedCAP as GCAP\n",
    "from Metrics import CZeroCAP as CZCAP\n",
    "from Metrics import DCR\n",
    "from Metrics import NNDR\n",
    "from Metrics import Hidden_rate\n",
    "from Metrics import NNAA\n",
    "from Metrics import MemInf as MIR\n",
    "from Metrics import Hitting_rate\n",
    "from Metrics import MDCR\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from Metrics import All_synthcity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "def get_metric_results(real_data, syn_data):\n",
    "    all_data = pd.concat([real_data, syn_data])\n",
    "    fn_encoder = LabelEncoder()\n",
    "    ln_encoder = LabelEncoder()\n",
    "    fl_encoder = LabelEncoder()\n",
    "    na_encoder = LabelEncoder()\n",
    "    r_fn = fn_encoder.fit_transform(all_data['First Name'])\n",
    "    r_ln = ln_encoder.fit_transform(all_data['Last Name'])\n",
    "    r_na = na_encoder.fit_transform(all_data['Nationality'])\n",
    "    r_fl = fl_encoder.fit_transform(all_data['Favorite Icecream'])\n",
    "    all_labels = pd.DataFrame({'First Name':r_fn, 'Last Name': r_ln, 'Height': all_data['Height'],'Nationality': r_na, 'Favorite Icecream':r_fl, 'Like Liquorice': all_data['Like Liquorice'], 'Times Been to Italy': all_data['Times Been to Italy'], 'First Time London': all_data['First Time London'], 'Steps per Day': all_data['Steps per Day']})\n",
    "    real_labels = all_labels[:len(real_data)]\n",
    "    syn_labels = all_labels[-len(real_data):]\n",
    "    \n",
    "    metrics = {\n",
    "                    'sanity': ['common_rows_proportion', 'nearest_syn_neighbor_distance', 'close_values_probability', 'distant_values_probability'],\n",
    "                    'stats': ['alpha_precision'],\n",
    "                    'detection': ['detection_mlp'],\n",
    "                    'privacy': ['identifiability_score'],\n",
    "                }\n",
    "    air = AIR.calculate_metric(args = None, _real_data=real_data, _synthetic=syn_data)\n",
    "    synthcity_results = All_synthcity.calculate_metric(args = None, _real_data=real_labels, _synthetic=syn_labels, _metrics=metrics)\n",
    "    crp = synthcity_results['mean'][1]\n",
    "    nsnd = 1-synthcity_results['mean'][2]\n",
    "    cvp = synthcity_results['mean'][3]\n",
    "    dvp = 1-synthcity_results['mean'][4]\n",
    "    auth = synthcity_results['mean'][10]\n",
    "    mlp = synthcity_results['mean'][11]\n",
    "    id_score = synthcity_results['mean'][12]\n",
    "    #air = AIR.calculate_metric(args = None, _real_data=real_data, _synthetic=syn_data)\n",
    "    gcap = GCAP.calculate_metric(args = None, _real_data=real_labels, _synthetic=syn_labels)\n",
    "    zcap = CZCAP.calculate_metric(args = None, _real_data=real_labels, _synthetic=syn_labels)\n",
    "    mdcr = MDCR.calculate_metric(args=None, _real_data=real_labels, _synthetic=syn_labels)\n",
    "    hitR = Hitting_rate.calculate_metric(args=None, _real_data=real_labels, _synthetic=syn_labels)\n",
    "    mir = MIR.calculate_metric(args=None, _real_data=real_labels, _synthetic=syn_labels)\n",
    "    nnaa = NNAA.calculate_metric(args=None, _real_data=real_labels, _synthetic=syn_labels)\n",
    "    #---These metrics simply take too long to run\n",
    "    # dcr = DCR.calculate_metric(args=None, _real_data=real_labels, _synthetic=syn_labels)\n",
    "    # nndr = NNDR.calculate_metric(args=None, _real_data=real_labels, _synthetic=syn_labels)\n",
    "    # hidd = Hidden_rate.calculate_metric(args=None, _real_data=real_labels, _synthetic=syn_labels)\n",
    "     \n",
    "    priv_results = np.around([air, gcap, zcap, \n",
    "                            mdcr, hitR, mir, \n",
    "                            nnaa, crp, nsnd, \n",
    "                            cvp, dvp, auth, \n",
    "                            mlp, id_score, \n",
    "                            #dcr, nndr, hidd\n",
    "                            ], 2).tolist()\n",
    "    \n",
    "    metric_list = [\"Attribute Inference Riks\", \"GeneralizedCAP\", \"ZeroCAP\", \n",
    "                   \"Median Distance to Closest Record\", \"Hitting Rate\",\n",
    "                   \"Membership Inference Risk\", \"Nearest Neighbour Adversarial Accuracy\",\n",
    "                   \"Common Row Proportion\", \"Nearest Synthetic Neighbour Distance\",\n",
    "                   \"Close Value Probability\", \"Distant Value Probability\",\n",
    "                   \"Authenticity\", \"DetectionMLP\", \"Identifiability Score\"\n",
    "                   #, \"Distance to Closest Record\", \"Nearest Neighbour Distance Ratio\", \"Hidden Rate\"\n",
    "                   ]\n",
    "    \n",
    "    results = pd.DataFrame({'Metric':metric_list, 'Result':priv_results})\n",
    "    \n",
    "    return results\n",
    "    \n",
    "\n",
    "eps_list = [0.02, 0.05, 0.1, 0.2, 0.5, 1, 2.5, 5]\n",
    "for liquorice in [0,1]:\n",
    "    real_dir = f'sample_data_{liquorice}.csv'\n",
    "    real_data = pd.read_csv(real_dir)\n",
    "    for eps in eps_list:\n",
    "        syn_no = pd.read_csv(f\"demo_syn/syn_no_{liquorice}_{eps}.csv\")\n",
    "        syn_bin = pd.read_csv(f\"demo_syn/syn_bin_{liquorice}_{eps}.csv\")\n",
    "        get_metric_results(real_data, syn_no).to_csv(f\"metric_full_new/syn_bin_{liquorice}_{eps}.csv\")\n",
    "        get_metric_results(real_data, syn_bin).to_csv(f\"metric_full_new/syn_no_{liquorice}_{eps}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "eps_list = [0.02, 0.05, 0.1, 0.2, 0.5, 1, 2.5, 5]\n",
    "for liquorice in [0,1]:\n",
    "    for eps in eps_list:\n",
    "        syn_no = pd.read_csv(f\"metric_full_new/syn_no_{liquorice}_{eps}.csv\")\n",
    "        syn_bin = pd.read_csv(f\"metric_full_new/syn_bin_{liquorice}_{eps}.csv\")\n",
    "        syn_no_more = pd.read_csv(f\"metric_results/syn_no_{liquorice}_{eps}.csv\")\n",
    "        syn_bin_more = pd.read_csv(f\"metric_results/syn_bin_{liquorice}_{eps}.csv\")\n",
    "        # full_no_bin.to_csv(f\"metric_results/syn_no_{liquorice}_{eps}.csv\")\n",
    "        # full_bin.to_csv(f\"metric_results/syn_bin_{liquorice}_{eps}.csv\")\n",
    "        \n",
    "        syn_no_more[\"Metric\"] = syn_no_more[\"Metric\"].str.strip()\n",
    "        syn_no[\"Metric\"] = syn_no[\"Metric\"].str.strip()\n",
    "        result_map = dict(zip(syn_no[\"Metric\"], syn_no[\"Result\"]))\n",
    "        syn_no_more[\"Result\"] = syn_no_more[\"Metric\"].map(result_map).combine_first(syn_no_more[\"Result\"])\n",
    "        syn_no_more.drop(columns=['Unnamed: 0.1']).to_csv(f\"metric_results/syn_no_{liquorice}_{eps}.csv\")\n",
    "        \n",
    "        syn_bin_more[\"Metric\"] = syn_bin_more[\"Metric\"].str.strip()\n",
    "        syn_bin[\"Metric\"] = syn_bin[\"Metric\"].str.strip()\n",
    "        result_map = dict(zip(syn_bin[\"Metric\"], syn_bin[\"Result\"]))\n",
    "        syn_bin_more[\"Result\"] = syn_bin_more[\"Metric\"].map(result_map).combine_first(syn_bin_more[\"Result\"])\n",
    "        syn_bin_more.drop(columns=['Unnamed: 0.1']).to_csv(f\"metric_results/syn_bin_{liquorice}_{eps}.csv\")\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
